# ENV args

[env]
time_limit = 25
time_step = 0.25
val_size = 50
test_size = 500
randomize_attributes = true


[reward]
success_reward = 2.5
collision_penalty = -0.1
discomfort_dist = 0.2
discomfort_penalty_factor = 0.5
step_reward = 0.005


[sim]
square_width = 10
human_num = 5


[humans]
visible = true
policy = orca
radius = 0.3
v_pref = 1


[robot]
visible = false
policy = none
radius = 0.3
v_pref = 1


[rl]
gamma = 0.9
mlp_dims = 64, 64, 64, 1
multiagent_training = true


[action_space]
# kinematics = holonomic
kinematics = unicycle
# action space size is speed_samples * rotation_samples + 1
speed_samples = 3
rotation_samples = 8


# Training args

[train]
batch_size = 64
learning_rate = 0.001
# number of batches to train at the end of training episode
train_batches = 100
train_episodes = 10000
sample_episodes = 1
target_update_interval = 50
evaluation_interval = 1000
capacity = 100000
epsilon_start = 1.0
epsilon_end = 0.1
epsilon_decay = 5000
checkpoint_interval = 100
